{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hdtruong802/Pytorch-Assignments/blob/main/%5BNLP%5DPart1_Build_Vocabulary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dự án thực hành: Phần 1 - Xây dựng bộ từ điển từ văn bản lớn\n",
        "```\n",
        "ProtonX - Pytorch Class\n",
        "```\n",
        "\n",
        "### Hướng dẫn làm bài\n",
        "- Trong bài tập này bạn sẽ sử dụng Python 3.\n",
        "- Sau khi bạn viết Code của mình xong, hãy chạy dòng Code đó để xem kết quả bên dưới.\n",
        "\n",
        "### [Quan trọng] Chú ý\n",
        "- **Không sử dụng hàm `input()` tại bất kỳ dòng lệnh nào**\n",
        "- **Không thay đổi dòng code return của hàm**\n",
        "\n",
        "Các bạn sẽ thực hiện `code` trong các phần hiển thị `#TODO: Lập trình tại đây` và thay thế các vị trí `None`. Có những câu hỏi chỉ cần trả về đáp án.\n",
        "\n",
        "Sau khi viết xong Code của bạn, bạn hãy ấn \"SHIFT\"+\"ENTER\" để thực hiện chạy lệnh của Cell đó.\n",
        "\n",
        "---\n",
        "Điểm số:\n",
        "* 10 điểm / Câu\n",
        "\n",
        "Tiêu chí chấm điểm:\n",
        "* Các bài tập sẽ được chấm dựa trên các Test-case.\n",
        "* Các bạn không khởi tạo lại giá trị đầu vào bên trong hàm. Có thể khởi tạo các giá trị này ngoài hàm nhằm mục đích kiểm thử."
      ],
      "metadata": {
        "id": "b6juhql4uLKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cài đặt bộ tokenizer bên thứ 3**\n",
        "\n",
        "Xem thêm về các bộ tách từ xây dựng bằng [Spacy](https://protonx.io/courses/6523788337e7f90012890145/topics/6571b351e5bfd8001a8f0d50)."
      ],
      "metadata": {
        "id": "2KFT6U5jgBND"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYDTzb8scUSW",
        "outputId": "c903aede-1c40-4576-f902-bff391d8fbb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyvi in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pyvi) (1.2.2)\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.10/dist-packages (from pyvi) (0.3.6)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (3.2.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (0.9.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyvi\n",
        "from spacy.lang.vi import Vietnamese"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thử tokenizer này"
      ],
      "metadata": {
        "id": "HJpTgW0xfkZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = Vietnamese()\n",
        "tokens = nlp('Lớp học Pytorch là lớp học rất hay ho')"
      ],
      "metadata": {
        "id": "2IBG31xffkGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5Zbw7oYfw74",
        "outputId": "157401a1-faf2-49ee-a6af-ba679f7e27e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Lớp học Pytorch là lớp học rất hay ho"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lặp qua các đơn vị lưu trữ trong Vocab dưới dạng từ vựng (Lexeme). Một từ vựng có các thuộc tính sau:"
      ],
      "metadata": {
        "id": "BPFDAXXngJ6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in tokens:\n",
        "    lexeme = tokens.vocab[word.text]\n",
        "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
        "            lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWw1WMRzgJSI",
        "outputId": "17a53f01-c3fb-4321-f954-c6c8ab66a01b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lớp học 9560392879819997953 Xxx xxx L học False False False vi\n",
            "Pytorch 16295890598922194816 Xxxxx P rch True False True vi\n",
            "là 12844001651356581174 xx l là True False False vi\n",
            "lớp học 16939850666758009144 xxx xxx l học False False False vi\n",
            "rất 1932685823275808203 xxx r rất True False False vi\n",
            "hay ho 3393752619603724165 xxx xx h  ho False False False vi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Văn bản: Văn bản gốc của từ vựng.\n",
        "\n",
        "- Orth: Giá trị băm của từ vựng.\n",
        "\n",
        "- Shape: Hình dạng từ ngữ trừu tượng của từ vựng.\n",
        "\n",
        "- Prefix: Theo mặc định, là chữ cái đầu tiên của chuỗi từ.\n",
        "\n",
        "- Suffix: Theo mặc định, là ba chữ cái cuối cùng của chuỗi từ.\n",
        "\n",
        "- is alpha: Liệu từ vựng có bao gồm các ký tự chữ cái không?\n",
        "\n",
        "- is digit: Liệu từ vựng có bao gồm các chữ số không?"
      ],
      "metadata": {
        "id": "39eBBiwsgVu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tải dữ liệu wiki"
      ],
      "metadata": {
        "id": "mlIyrq0cgxdr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bạn hãy tải dữ liệu Wiki [tại đây](https://drive.google.com/file/d/1f0yEMqH7U5v4uhGHcRHxRWUhOndV9xkn/view?usp=drive_link) và upload lên Colab"
      ],
      "metadata": {
        "id": "bAZfAjPPlirz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phân tích: Có rất nhiều câu ngắn trong dữ liệu vì thế bạn hãy lọc những câu có chiều dài ngắn hơn 10 từ và tiến hành tách token"
      ],
      "metadata": {
        "id": "qy02qpKcl4cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhS9qUZAsn43",
        "outputId": "b1303b5e-8fc8-42b0-8b21-fac893e0c23d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import io\n",
        "\n",
        "def count_tokens_in_file(filepath, min_length):\n",
        "    \"\"\"\n",
        "    Hàm này đọc một tệp tin văn bản và đếm số lượng xuất hiện của từng token.\n",
        "    Chỉ xử lý các câu có chiều dài lớn hơn hoặc bằng min_length.\n",
        "\n",
        "    Args:\n",
        "    filepath: Đường dẫn đến tệp tin văn bản.\n",
        "    min_length: Độ dài tối thiểu của câu để được xử lý.\n",
        "\n",
        "    Returns:\n",
        "    Counter object với số lượng xuất hiện của mỗi token.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO 1: Bộ đếm token và số lượng xuất hiện\n",
        "    counter = Counter()\n",
        "    # Mở File\n",
        "    with io.open(filepath, encoding=\"utf8\") as f:\n",
        "        for string_ in f:\n",
        "            # TODO 2: Chỉ làm việc với những câu có chiều dài lớn hơn hoặc bằng min_length\n",
        "            if len(string_) >= min_length:\n",
        "              # Tiến hành tách token\n",
        "              doc = set(string_.replace(\"\\n\",\"\").replace(\",\",\"\").replace('\"',\"\").replace(\".\",\"\").split(\" \"))\n",
        "              # TODO 3: Update counter với bộ đếm\n",
        "              counter.update(doc)\n",
        "    return counter\n",
        "\n",
        "# Sử dụng hàm\n",
        "filepath = '/content/drive/My Drive/wiki_part1.txt'\n",
        "min_length = 10\n",
        "# TODO 4: Dùng hàm count_tokens_in_file để build counter\n",
        "counter = count_tokens_in_file(filepath, min_length)"
      ],
      "metadata": {
        "id": "FfK5ZAycqUr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cài đặt các ký tự đặc biệt:"
      ],
      "metadata": {
        "id": "MXV0cl11mSip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Token không có\n",
        "unk_token = '<unk>'\n",
        "# Token để padding\n",
        "pad_token = '<pad>'\n",
        "# Token bắt đầu câu\n",
        "bos_token = '<bos>'\n",
        "# Token kết thúc câu\n",
        "eos_token = '<eos>'\n",
        "\n",
        "from torchtext.vocab import vocab\n",
        "# TODO 5: Xây dựng Vocab bằng Pytorch\n",
        "torchVocab = vocab(counter, specials=[unk_token, pad_token, bos_token, eos_token])"
      ],
      "metadata": {
        "id": "7oFLRk5Ohgv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Code"
      ],
      "metadata": {
        "id": "4-KT04cwm4TO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_initialization():\n",
        "    try:\n",
        "        assert unk_token in torchVocab.get_stoi()\n",
        "        assert pad_token in torchVocab.get_stoi()\n",
        "        assert bos_token in torchVocab.get_stoi()\n",
        "        assert eos_token in torchVocab.get_stoi()\n",
        "        print(\"Test Initialization: Passed\")\n",
        "    except AssertionError:\n",
        "        print(\"Test Initialization: Failed\")\n",
        "\n",
        "def test_special_tokens():\n",
        "    try:\n",
        "        for token in [unk_token, pad_token, bos_token, eos_token]:\n",
        "            assert token in torchVocab.get_stoi()\n",
        "        print(\"Test Special Tokens: Passed\")\n",
        "    except AssertionError:\n",
        "        print(\"Test Special Tokens: Failed\")\n",
        "\n",
        "def test_token_to_index():\n",
        "    try:\n",
        "        known_token = list(counter.keys())[0]\n",
        "        assert torchVocab[known_token] == torchVocab.get_stoi()[known_token]\n",
        "        assert torchVocab['<unk>'] == torchVocab.get_stoi()[str(unk_token)]\n",
        "        print(\"Test Token to Index Conversion: Passed\")\n",
        "    except AssertionError:\n",
        "        print(\"Test Token to Index Conversion: Failed\")\n",
        "\n",
        "def test_index_to_token():\n",
        "    try:\n",
        "        known_index = torchVocab.get_stoi()[list(counter.keys())[0]]\n",
        "        assert torchVocab.get_itos()[known_index] == list(counter.keys())[0]\n",
        "        print(\"Test Index to Token Conversion: Passed\")\n",
        "    except AssertionError:\n",
        "        print(\"Test Index to Token Conversion: Failed\")\n",
        "\n",
        "def test_vocabulary_size():\n",
        "    try:\n",
        "        expected_size = len(counter) + 4  # 4 for the special tokens\n",
        "        assert len(torchVocab) == expected_size\n",
        "        print(\"Test Vocabulary Size: Passed\")\n",
        "    except AssertionError:\n",
        "        print(\"Test Vocabulary Size: Failed\")\n",
        "\n",
        "def test_unknown_token_handling():\n",
        "    try:\n",
        "        assert torchVocab['<unk>'] == torchVocab.get_stoi()[unk_token]\n",
        "        print(\"Test Handling of Unknown Tokens: Passed\")\n",
        "    except AssertionError:\n",
        "        print(\"Test Handling of Unknown Tokens: Failed\")\n",
        "\n",
        "\n",
        "test_initialization()\n",
        "test_special_tokens()\n",
        "test_token_to_index()\n",
        "test_index_to_token()\n",
        "test_vocabulary_size()\n",
        "test_unknown_token_handling()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9ehpKmSm2gl",
        "outputId": "ddeab1df-0c49-4f5d-9606-8bb1fc41bd5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Initialization: Passed\n",
            "Test Special Tokens: Passed\n",
            "Test Token to Index Conversion: Passed\n",
            "Test Index to Token Conversion: Passed\n",
            "Test Vocabulary Size: Passed\n",
            "Test Handling of Unknown Tokens: Passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kết quả mong đợi:**\n",
        "\n",
        "```\n",
        "Test Initialization: Passed\n",
        "Test Special Tokens: Passed\n",
        "Test Token to Index Conversion: Passed\n",
        "Test Index to Token Conversion: Passed\n",
        "Test Vocabulary Size: Passed\n",
        "Test Handling of Unknown Tokens: Passed\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "mLmVK5D1ns7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 4\n",
        "PAD_IDX = torchVocab['<pad>']\n",
        "BOS_IDX = torchVocab['<bos>']\n",
        "EOS_IDX = torchVocab['<eos>']"
      ],
      "metadata": {
        "id": "uR58dHjOnUDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lập trình hàm cắt một chuỗi thành nhiều chuỗi nhỏ"
      ],
      "metadata": {
        "id": "DTjde8S2oCXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm `trim_sequence_to_parts` có chức năng chia một chuỗi (hoặc một dãy các phần tử) thành nhiều phần nhỏ với độ dài xác định. Dưới đây là mô tả chi tiết về các đối số đầu vào và giá trị trả về của hàm này:\n",
        "\n",
        "**Đối số:**\n",
        "1. `sequence`: Đây là chuỗi hoặc dãy các phần tử cần được chia nhỏ. Đối số này có thể là một chuỗi các ký tự, một danh sách, hoặc bất kỳ đối tượng nào khác có thể được lặp qua (iterable).\n",
        "2. `part_length`: Đây là độ dài của từng phần sau khi chia. Đối số này phải là một số nguyên, chỉ ra số lượng phần tử tối đa trong mỗi phần nhỏ sau khi chia.\n",
        "\n",
        "**Chức năng của Hàm:**\n",
        "- Hàm `trim_sequence_to_parts` lặp qua `sequence` và chia nó thành các phần nhỏ, mỗi phần có độ dài không vượt quá `part_length`.\n",
        "- Việc chia này được thực hiện bằng cách sử dụng list comprehension, một cấu trúc ngắn gọn trong Python để tạo ra các danh sách mới.\n",
        "\n",
        "**Giá trị trả về:**\n",
        "- Hàm trả về một danh sách mới, trong đó mỗi phần tử là một phần của `sequence` với độ dài không vượt quá `part_length`.\n",
        "- Nếu `sequence` không thể chia đều cho `part_length`, phần cuối cùng của danh sách trả về có thể sẽ ngắn hơn.\n",
        "\n",
        "**Ví dụ:**\n",
        "Nếu bạn gọi `trim_sequence_to_parts([1, 2, 3, 4, 5, 6, 7], 3)`, hàm sẽ trả về `[[1, 2, 3], [4, 5, 6], [7]]`. Trong ví dụ này, chuỗi `[1, 2, 3, 4, 5, 6, 7]` được chia thành các phần, mỗi phần có 3 phần tử, trừ phần cuối cùng chỉ có 1 phần tử."
      ],
      "metadata": {
        "id": "T6hPxrhuoZ47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 30\n",
        "def trim_sequence_to_parts(sequence, part_length):\n",
        "    # TODO 6: Lập trình tại đây\n",
        "    result = []\n",
        "    for i in range(0, len(sequence), part_length):\n",
        "        result.append(sequence[i:i+part_length])\n",
        "    return result"
      ],
      "metadata": {
        "id": "XCSRbTi3oE0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trim_sequence_to_parts([1, 2, 3, 4, 5, 6, 7], 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT457yVP0eYL",
        "outputId": "2409ec93-09ef-4173-d187-47e4d5a1965e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3], [4, 5, 6], [7]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lập trình hàm xử lý dữ liệu\n",
        "\n",
        "Hàm `data_process` là một hàm dùng để xử lý dữ liệu văn bản từ một tệp tin và chuyển đổi chúng thành dạng tensor sử dụng một từ điển (vocab). Dưới đây là mô tả chi tiết về các đối số và giá trị trả về của hàm này:\n",
        "\n",
        "**Đối số:**\n",
        "1. `filepath`: Đường dẫn đến tệp tin chứa dữ liệu văn bản cần xử lý. Đây là một chuỗi ký tự (string) chỉ ra vị trí của tệp tin trong hệ thống tập tin.\n",
        "2. `vocab`: Một từ điển hoặc cấu trúc dữ liệu tương tự, dùng để chuyển đổi từ các token văn bản sang số nguyên (index). Mỗi token trong văn bản sẽ được tìm kiếm trong `vocab` để lấy chỉ số tương ứng.\n",
        "3. `min_length`: Độ dài tối thiểu của một chuỗi ký tự (câu) để nó được xem xét và xử lý. Nếu một chuỗi ngắn hơn `min_length`, nó sẽ bị bỏ qua.\n",
        "4. `max_length`: Độ dài tối đa cho phép của một chuỗi tensor sau khi được chia nhỏ bằng hàm `trim_sequence_to_parts`.\n",
        "\n",
        "**Chức năng của Hàm:**\n",
        "- Hàm mở tệp tin văn bản tại `filepath` sử dụng mã hóa UTF-8.\n",
        "- Đọc từng dòng trong tệp tin. Mỗi dòng được coi như một chuỗi ký tự (`string_`).\n",
        "- Nếu độ dài của `string_` (tính theo số lượng từ) lớn hơn hoặc bằng `min_length`, chuỗi sẽ được xử lý tiếp:\n",
        "    - Sử dụng một hàm nlp như trên để xử lý chuỗi ký tự này, tạo ra một đối tượng (có thể là một danh sách các token).\n",
        "    - Mỗi token trong đối tượng này được chuyển đổi thành số nguyên (index) sử dụng `vocab`. Kết quả là một tensor PyTorch (`vi_tensor_`).\n",
        "    - Tensor này sau đó được chia thành nhiều phần nhỏ hơn với độ dài tối đa là `max_length` sử dụng hàm `trim_sequence_to_parts`.\n",
        "    - Mỗi phần nhỏ này được thêm vào danh sách `data`.\n",
        "\n",
        "**Giá trị trả về:**\n",
        "- Hàm trả về `data`, là một danh sách các phần tensor. Mỗi phần tensor đại diện cho một phần của dữ liệu văn bản, đã được chuyển đổi thành dạng số nguyên và chia theo độ dài tối đa xác định.\n",
        "\n"
      ],
      "metadata": {
        "id": "gTX_YN5kovww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import traceback\n",
        "\n",
        "def data_process(filepath, vocab, min_length, max_length):\n",
        "    data = []\n",
        "    # TODO 7: Lập trình tại đây\n",
        "    try:\n",
        "      with open(filepath, 'r', encoding=\"utf8\") as f:\n",
        "        for string_ in f:\n",
        "          string_ = string_.strip()\n",
        "          tokens = [token for token in string_.split() if token]\n",
        "          if len(tokens) >= min_length:\n",
        "            # Chuyển đổi token thành index sử dụng vocab\n",
        "            indexes = [vocab[token] for token in tokens if token in vocab]\n",
        "            # Tạo tensor PyTorch từ các index\n",
        "            vi_tensor_ = torch.tensor(indexes, dtype=torch.long)\n",
        "            # Chia tensor thành các phần nhỏ hơn với độ dài tối đa là max_length\n",
        "            parts = trim_sequence_to_parts(vi_tensor_, max_length)\n",
        "            # Thêm các phần nhỏ vào danh sách data\n",
        "            data.extend(parts)\n",
        "    except Exception as e:\n",
        "      traceback.print_exc()\n",
        "    return data"
      ],
      "metadata": {
        "id": "qWrE1bt0n6NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = data_process(filepath, torchVocab, 10, 30)"
      ],
      "metadata": {
        "id": "lI-oQfeWPUHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Code"
      ],
      "metadata": {
        "id": "CVvSEQ21pLLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test.txt\n",
        "Những gì bạn sẽ học được từ lớp học này\n",
        "\n",
        "Hiểu cách các mô hình học sâu hoạt động\n",
        "\n",
        "Triển khai các model tiên tiến nhất với Pytorch bao gồm thị giác máy tính và xử lý ngôn ngữ tự nhiên\n",
        "\n",
        "Cách làm việc với nhiều loại dữ liệu khác nhau để xây dựng model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLh5_uDypJK-",
        "outputId": "be98e855-6122-4580-b176-e601b9100eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_test_path = 'test.txt'\n",
        "test_counter = count_tokens_in_file(file_test_path, min_length)\n",
        "testVocab = vocab(test_counter, specials=[unk_token, pad_token , bos_token, eos_token])\n",
        "test_data = data_process(file_test_path, testVocab, 10, 30)"
      ],
      "metadata": {
        "id": "ZRm1AFMvpSqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQLNO9tGrb9L",
        "outputId": "2f44a03e-c0bc-4bd6-8550-340761a30716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'sẽ': 1,\n",
              "         'bạn': 1,\n",
              "         'này': 1,\n",
              "         'được': 1,\n",
              "         'gì': 1,\n",
              "         'Những': 1,\n",
              "         'từ': 1,\n",
              "         'lớp': 1,\n",
              "         'học': 2,\n",
              "         'sâu': 1,\n",
              "         'mô': 1,\n",
              "         'Hiểu': 1,\n",
              "         'các': 2,\n",
              "         'cách': 1,\n",
              "         'hoạt': 1,\n",
              "         'hình': 1,\n",
              "         'động': 1,\n",
              "         'bao': 1,\n",
              "         'gồm': 1,\n",
              "         'model': 2,\n",
              "         'nhất': 1,\n",
              "         'lý': 1,\n",
              "         'khai': 1,\n",
              "         'nhiên': 1,\n",
              "         'tính': 1,\n",
              "         'thị': 1,\n",
              "         'giác': 1,\n",
              "         'ngôn': 1,\n",
              "         'Triển': 1,\n",
              "         'Pytorch': 1,\n",
              "         'và': 1,\n",
              "         'tự': 1,\n",
              "         'tiến': 1,\n",
              "         'ngữ': 1,\n",
              "         'máy': 1,\n",
              "         'với': 2,\n",
              "         'xử': 1,\n",
              "         'tiên': 1,\n",
              "         'dữ': 1,\n",
              "         'nhiều': 1,\n",
              "         'làm': 1,\n",
              "         'loại': 1,\n",
              "         'liệu': 1,\n",
              "         'khác': 1,\n",
              "         'Cách': 1,\n",
              "         'để': 1,\n",
              "         'việc': 1,\n",
              "         'dựng': 1,\n",
              "         'nhau': 1,\n",
              "         'xây': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l873evBSq5ta",
        "outputId": "e6ab007f-e3ff-400c-d622-c79601d12eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([ 9,  8,  5,  4, 12,  7, 10, 11, 12,  6]),\n",
              " tensor([32, 26, 16, 23, 41, 36, 24, 39, 33, 21, 22, 29, 30, 38, 28, 34, 40, 25,\n",
              "         31, 37, 35, 27]),\n",
              " tensor([48, 44, 50, 39, 43, 45, 42, 46, 47, 52, 49, 53, 51, 23])]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kết quả mong đợi\n",
        "\n",
        "```python\n",
        "[tensor([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13]),\n",
        " tensor([13]),\n",
        " tensor([14, 15, 16, 17, 18, 19, 13]),\n",
        " tensor([13]),\n",
        " tensor([20, 16, 21, 22, 23, 24, 25, 26, 27, 28]),\n",
        " tensor([29, 30, 31, 32, 13]),\n",
        " tensor([13]),\n",
        " tensor([33, 34, 24, 35, 36, 37, 38, 39, 40, 41]),\n",
        " tensor([21, 13])]\n",
        "```"
      ],
      "metadata": {
        "id": "P2HH4KF5sUmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4\n",
        "PAD_IDX = torchVocab['<pad>']\n",
        "BOS_IDX = torchVocab['<bos>']\n",
        "EOS_IDX = torchVocab['<eos>']"
      ],
      "metadata": {
        "id": "OsVE3qe2s_oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lập trình hàm chia batch\n",
        "\n",
        "Hàm `generate_batch` là một hàm được thiết kế để xử lý các batch dữ liệu (batch) và chuẩn bị chúng để có thể được sử dụng trong các mô hình học máy, đặc biệt là trong xử lý ngôn ngữ tự nhiên. Dưới đây là mô tả chi tiết về các đối số và giá trị trả về của hàm này:\n",
        "\n",
        "**Đối số:**\n",
        "1. `data_batch`: Một batch dữ liệu, thường là một danh sách các tensor. Mỗi tensor trong `data_batch` đại diện cho một mẫu dữ liệu (ví dụ, một câu hoặc đoạn văn).\n",
        "\n",
        "**Chức năng của Hàm:**\n",
        "- Hàm lặp qua mỗi mẫu dữ liệu (`vi_item`) trong `data_batch`.\n",
        "- Đối với mỗi `vi_item`:\n",
        "  - Tạo một tensor `pads` chứa các giá trị padding (PAD_IDX) với kích thước sao cho tổng chiều dài của `vi_item` sau khi thêm các giá trị padding sẽ bằng `max_length`.\n",
        "  - Sử dụng hàm `torch.cat` để ghép `vi_item` với các giá trị BOS_IDX (đánh dấu bắt đầu của chuỗi), EOS_IDX (đánh dấu kết thúc của chuỗi), và tensor `pads` chứa các giá trị padding.\n",
        "  - Thêm `vi_item` đã được xử lý vào danh sách `vi_batch`.\n",
        "- Cuối cùng, sử dụng hàm `pad_sequence` để chuẩn hóa kích thước của tất cả các mẫu trong `vi_batch`, đảm bảo rằng tất cả chúng có chiều dài như nhau bằng cách thêm các giá trị padding (PAD_IDX) ở cuối nếu cần.\n",
        "\n",
        "**Giá trị trả về:**\n",
        "- Hàm trả về `vi_batch`, là một tensor đã được chuẩn hóa về kích thước, với mỗi mẫu dữ liệu trong batch đều có cùng chiều dài `max_length`.\n"
      ],
      "metadata": {
        "id": "UQJ3CNybOK5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "  vi_batch = []\n",
        "  # TODO 8: Lập trình tại đây\n",
        "  for vi_item in data_batch:\n",
        "    #Tạo một tensor pads chứa các giá trị padding (PAD_IDX)\n",
        "    pads = torch.full((max_length - len(vi_item),), PAD_IDX, dtype=torch.long)\n",
        "    #Sử dụng hàm torch.cat để ghép vi_item với các giá trị BOS_IDX (đánh dấu bắt đầu của chuỗi), EOS_IDX (đánh dấu kết thúc của chuỗi), và tensor pads chứa các giá trị padding.\n",
        "    vi_item = torch.cat([torch.tensor([BOS_IDX]), vi_item, torch.tensor([EOS_IDX]), pads])\n",
        "    #Thêm vi_item đã được xử lý vào danh sách vi_batch.\n",
        "    vi_batch.append(vi_item)\n",
        "  #Chuẩn hóa tất cả các mẫu\n",
        "  vi_batch = pad_sequence(vi_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "  return vi_batch"
      ],
      "metadata": {
        "id": "tJ_UbVKxtAl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Code"
      ],
      "metadata": {
        "id": "IMLhBuR6O4tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_generate_batch():\n",
        "    try:\n",
        "        # Test Case 1: Single item in data_batch\n",
        "        data_batch = [torch.tensor([3, 4, 5])]\n",
        "        batch = generate_batch(data_batch)\n",
        "        assert batch.shape == (1, 32), \"Failed: Shape mismatch in single item test\"\n",
        "\n",
        "        # Test Case 2: Multiple items in data_batch of different lengths\n",
        "        data_batch = [torch.tensor([3, 4, 5]), torch.tensor([6, 7])]\n",
        "        batch = generate_batch(data_batch)\n",
        "        assert batch.shape == (2, 32), \"Failed: Shape mismatch in multiple items test\"\n",
        "\n",
        "        # Test Case 3: Check if BOS_IDX and EOS_IDX are added correctly\n",
        "        data_batch = [torch.tensor([3])]\n",
        "        batch = generate_batch(data_batch)\n",
        "        assert batch[0][0] == BOS_IDX and batch[0][2] == EOS_IDX, \"Failed: BOS_IDX or EOS_IDX missing\"\n",
        "\n",
        "        print(\"All tests passed.\")\n",
        "    except AssertionError as e:\n",
        "        print(e)\n",
        "\n",
        "# Run the tests\n",
        "test_generate_batch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bepj_0MNPrtD",
        "outputId": "5488228f-8c0c-48c9-db67-845da200fe02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kết quả mong đợi:**\n",
        "\n",
        "All tests passed.\n"
      ],
      "metadata": {
        "id": "J3cGc4c-QS2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "test_iter = DataLoader(test_data, batch_size=4,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "for item in test_iter:\n",
        "    print('Chiều: ', item.shape)\n",
        "    print('Giá trị')\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvZIMo-stSRw",
        "outputId": "f4d388b0-2f92-4f13-8a63-c9c32a19f088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chiều:  torch.Size([3, 32])\n",
            "Giá trị\n",
            "tensor([[ 2,  9,  8,  5,  4, 12,  7, 10, 11, 12,  6,  3,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "        [ 2, 32, 26, 16, 23, 41, 36, 24, 39, 33, 21, 22, 29, 30, 38, 28, 34, 40,\n",
            "         25, 31, 37, 35, 27,  3,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "        [ 2, 48, 44, 50, 39, 43, 45, 42, 46, 47, 52, 49, 53, 51, 23,  3,  1,  1,\n",
            "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kết quả mong đợi:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Chiều:  torch.Size([4, 32])\n",
        "Giá trị\n",
        "tensor([[ 2, 14, 15, 16, 17, 18, 19, 13,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
        "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
        "        [ 2, 13,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
        "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
        "        [ 2, 20, 16, 21, 22, 23, 24, 25, 26, 27, 28,  3,  1,  1,  1,  1,  1,  1,\n",
        "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
        "        [ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  3,  1,  1,  1,  1,  1,  1,\n",
        "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]])\n",
        "Chiều:  torch.Size([4, 32])\n",
        "Giá trị\n",
        "tensor([[ 2, 21, 13,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
        "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
        "        [ 2, 29, 30, 31, 32, 13,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
        "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
        "        [ 2, 13,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
        "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
        "        [ 2, 13,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
        "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]])\n",
        "Chiều:  torch.Size([1, 32])\n",
        "Giá trị\n",
        "tensor([[ 2, 33, 34, 24, 35, 36, 37, 38, 39, 40, 41,  3,  1,  1,  1,  1,  1,  1,\n",
        "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "XVFY1L3Tto8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thực hiện build trên bộ wiki lớn"
      ],
      "metadata": {
        "id": "M99m1nfotuGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "train_iter = DataLoader(train_data, batch_size=8,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "# for item in train_iter:\n",
        "#     print('Chiều: ', item.shape)\n",
        "#     print('Giá trị')\n",
        "#     print(item)"
      ],
      "metadata": {
        "id": "BCeQyKbAtv9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save vocab"
      ],
      "metadata": {
        "id": "wKx4L2i9t0DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(torchVocab, 'wikiVocab.pth')"
      ],
      "metadata": {
        "id": "_7tJ3IV8tzxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load vocab"
      ],
      "metadata": {
        "id": "73vvZufNt8vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torchVocab = torch.load('wikiVocab.pth')"
      ],
      "metadata": {
        "id": "Hdxk7jEJt9s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lấy danh sách các phương thức của đối tượng Vocab\n",
        "vocab_methods = [method for method in dir(torchVocab) if callable(getattr(torchVocab, method))]\n",
        "\n",
        "# In ra các phương thức của đối tượng Vocab\n",
        "print(\"Các phương thức của đối tượng Vocab:\")\n",
        "for method in vocab_methods:\n",
        "    print(method)\n",
        "\n",
        "# Lấy danh sách các thuộc tính của đối tượng Vocab\n",
        "vocab_attributes = [attribute for attribute in dir(torchVocab) if not callable(getattr(torchVocab, attribute))]\n",
        "\n",
        "# In ra các thuộc tính của đối tượng Vocab\n",
        "print(\"Các thuộc tính của đối tượng Vocab:\")\n",
        "for attribute in vocab_attributes:\n",
        "    print(attribute)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRNtvjm8PloF",
        "outputId": "6201c4f6-77ac-48ea-8e38-40a8fd0c3105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Các phương thức của đối tượng Vocab:\n",
            "__call__\n",
            "__class__\n",
            "__contains__\n",
            "__delattr__\n",
            "__dir__\n",
            "__eq__\n",
            "__format__\n",
            "__ge__\n",
            "__getattr__\n",
            "__getattribute__\n",
            "__getitem__\n",
            "__getstate__\n",
            "__gt__\n",
            "__hash__\n",
            "__init__\n",
            "__init_subclass__\n",
            "__le__\n",
            "__len__\n",
            "__lt__\n",
            "__ne__\n",
            "__new__\n",
            "__prepare_scriptable__\n",
            "__reduce__\n",
            "__reduce_ex__\n",
            "__repr__\n",
            "__setattr__\n",
            "__setstate__\n",
            "__sizeof__\n",
            "__str__\n",
            "__subclasshook__\n",
            "_apply\n",
            "_call_impl\n",
            "_get_backward_hooks\n",
            "_get_backward_pre_hooks\n",
            "_get_name\n",
            "_load_from_state_dict\n",
            "_maybe_warn_non_full_backward_hook\n",
            "_named_members\n",
            "_register_load_state_dict_pre_hook\n",
            "_register_state_dict_hook\n",
            "_replicate_for_data_parallel\n",
            "_save_to_state_dict\n",
            "_slow_forward\n",
            "_wrapped_call_impl\n",
            "add_module\n",
            "append_token\n",
            "apply\n",
            "bfloat16\n",
            "buffers\n",
            "children\n",
            "compile\n",
            "cpu\n",
            "cuda\n",
            "double\n",
            "eval\n",
            "extra_repr\n",
            "float\n",
            "forward\n",
            "get_buffer\n",
            "get_default_index\n",
            "get_extra_state\n",
            "get_itos\n",
            "get_parameter\n",
            "get_stoi\n",
            "get_submodule\n",
            "half\n",
            "insert_token\n",
            "ipu\n",
            "load_state_dict\n",
            "lookup_indices\n",
            "lookup_token\n",
            "lookup_tokens\n",
            "modules\n",
            "named_buffers\n",
            "named_children\n",
            "named_modules\n",
            "named_parameters\n",
            "parameters\n",
            "register_backward_hook\n",
            "register_buffer\n",
            "register_forward_hook\n",
            "register_forward_pre_hook\n",
            "register_full_backward_hook\n",
            "register_full_backward_pre_hook\n",
            "register_load_state_dict_post_hook\n",
            "register_module\n",
            "register_parameter\n",
            "register_state_dict_pre_hook\n",
            "requires_grad_\n",
            "set_default_index\n",
            "set_extra_state\n",
            "share_memory\n",
            "state_dict\n",
            "to\n",
            "to_empty\n",
            "train\n",
            "type\n",
            "xpu\n",
            "zero_grad\n",
            "Các thuộc tính của đối tượng Vocab:\n",
            "T_destination\n",
            "__annotations__\n",
            "__dict__\n",
            "__doc__\n",
            "__jit_unused_properties__\n",
            "__module__\n",
            "__weakref__\n",
            "_backward_hooks\n",
            "_backward_pre_hooks\n",
            "_buffers\n",
            "_compiled_call_impl\n",
            "_forward_hooks\n",
            "_forward_hooks_always_called\n",
            "_forward_hooks_with_kwargs\n",
            "_forward_pre_hooks\n",
            "_forward_pre_hooks_with_kwargs\n",
            "_is_full_backward_hook\n",
            "_load_state_dict_post_hooks\n",
            "_load_state_dict_pre_hooks\n",
            "_modules\n",
            "_non_persistent_buffers_set\n",
            "_parameters\n",
            "_state_dict_hooks\n",
            "_state_dict_pre_hooks\n",
            "_version\n",
            "call_super_init\n",
            "dump_patches\n",
            "is_jitable\n",
            "training\n",
            "vocab\n"
          ]
        }
      ]
    }
  ]
}